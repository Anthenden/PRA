
\newif\ifwhole

\wholetrue
% Ajouter \wholetrue si on compile seulement ce fichier

\ifwhole
 \documentclass[a4page,10pt]{article}
     \input{entete}
 \begin{document}
\input{enonce}
\title{Plongement de Plücker}
\maketitle
\fi
Dans ce texte, on va expliquer comment réaliser la grassmannienne comme variété algébrique projective. \\
Référence : H2G2 tome II
\section{Algèbre extérieure}
Soit $E$ un espace vectoriel de dimension finie sur un corps commutatif $k$. \\
On appelle $p$ème puissance extérieure de $E$ l'espace vectoriel $\bigwedge^p E$,quotient de $E^{\otimes p}$ par le sous-espace vectoriel $\Zcal_p=\Vect(x_{1} \otimes \ldots \otimes x_{p} | x_i \in E,\exists m \neq n, x_{m}=x_{n})$ (par convention, $\bigwedge^0 E=k$). \\
On note $x_1 \wedge \ldots \wedge x_p$ la classe d'équivalence de $x_1 \otimes \ldots \otimes x_p$. \\
Le passage au quotient conserve les propriétés d'associativité et de $p$-linéarité. De plus, $\forall u,v \in E, u \wedge v=-v \wedge u$ et donc $\forall \sigma \in \Sfrak_r,u_1 \wedge \ldots \wedge u_r=\varepsilon(\sigma)u_{\sigma(1)} \wedge \ldots \wedge u_{\sigma(r)}$  \\
On appelle algèbre extérieure la somme directe des puissances extérieures : $\bigwedge^* E=\bigoplus_{p \in \N} \bigwedge^p E$. On munit $\bigwedge^* E$ d'une structure d'algèbre graduée grâce au produit : \\ $(x_1 \wedge \ldots x_p,y_1 \wedge \ldots \wedge y_q) \in \bigwedge^p E \times \bigwedge^q E \mapsto x_1 \wedge \ldots \wedge x_p \wedge y_1 \wedge \ldots \wedge y_q \in \bigwedge^{p+q} E$.


\begin{Lemme}
$\Zcal_p=\Vect(x_{1} \otimes \ldots \otimes x_{p} | x_i \in E, \{x_1,\ldots,x_p\} \text{ est une famille liée })$
\end{Lemme}
\begin{proof}
Posons $V=\Vect(x_{1} \otimes \ldots \otimes x_{p} | x_i \in E, \{x_1,\ldots,x_p\} \text{ est une famille liée })$. \\
On a clairement $\Zcal_p \subset V$. Réciproquement, soit $x_1 \wedge \ldots \wedge x_p \in V$ i.e.  $\{x_1,\ldots,x_p\}$ est liée. On a donc un $i\in [\![1,n]\!]$ et des $\lambda_j \in k,j \neq i$ tel que $x_i=\sum_{j \neq i} \lambda_jx_j$ et donc : 
 $x_1 \wedge \ldots \wedge x_i \wedge \ldots \wedge x_p=\sum_{j \neq i} \lambda_j x_1 \wedge \ldots \wedge x_j \wedge \ldots \wedge x_p \in \Zcal_p$ 
\end{proof}
Cela nous permet de voir que $\bigwedge^p E= \{0\}$ si $p>dim(E)$

\begin{Prop}
Soit $\Bcal=\{e_1,\ldots,e_n\}$ une base de $E$. Pour $p \leq n$,$\{e_{i_1} \wedge \ldots \wedge e_{i_p},1 \leq i_1<\ldots <i_p \leq n\}$ est une base de $\bigwedge^p E$.
\end{Prop}
\begin{proof}
Comme la famille $\{e_1 \otimes \ldots \otimes e_p|e_i \in \Bcal\}$ est une base de $E^{\otimes p}$ alors le passage au quotient fait que l'espace quotient a comme base $ \{e_1 \wedge \ldots \wedge e_p|e_i \in \Bcal, i \neq j \Rightarrow e_i \neq e_j\}$. En ordonnant les $e_i$; on obtient le résultat. 
\end{proof}

\begin{Cor}
$\dim_k \bigwedge^p E=\binom{\dim E}{p}$
\end{Cor}
En particulier, $\dim_k \bigwedge^{\dim E} E=1$ (cela permet de définir le déterminant)

\begin{Cor}
Soient $\{e_1,\ldots,e_m\},\{f_1,\ldots,f_m\}$ deux familles libres d'éléments de $E$. %Alors $\Vect(e_1,\ldots,e_m)=\Vect(f_1,\ldots,f_m)$ si, et seulement si, $e_1 \wedge \ldots \wedge e_m$ et $f_1 \wedge \ldots \wedge f_m$ sont colinéaires.
Si $\Vect(e_1,\ldots,e_m)=\Vect(f_1,\ldots,f_m)$ alors $e_1 \wedge \ldots \wedge e_m$ et $f_1 \wedge \ldots \wedge f_m$ sont colinéaires.
\end{Cor}

\begin{proof}
Soit $W=\Vect(e_1,\ldots,e_m)$. Alors $e_1 \wedge \ldots \wedge e_m$, $f_1 \wedge \ldots \wedge f_m \in \bigwedge^k W$. Ce dernier ensemble étant de dimension 1, $e_1 \wedge \ldots \wedge e_m$ et $f_1 \wedge \ldots \wedge f_m$ sont colinéaires.\\
%$\Leftarrow$ : Soit $W=\Vect(e_1,\ldots,e_m)$ et $W'=\Vect(f_1,\ldots,f_m)$. Si $f_i \notin W$ alors on complète $\{f_i,e_1,\ldots,e_m\}$ en une base $\{v_1,\ldots,v_n\}$ de $E$.
%Pour donner les idées, prenons $i=1$, \\
%$\lambda e_1 \wedge \ldots \wedge e_m=f_1 \wedge f_2 \wedge \ldots \wedge f_n=f_1 \wedge \sum_{j_2} a_{2j_2} v_{j_2} \wedge \ldots \wedge \sum_{j_m} a_{mj_m} v_{j_m}=\sum_{j_2}\cdots\sum_{j_m}f_1 \wedge  a_{2j_2} v_{j_2} \wedge \ldots \wedge  a_{mj_m} v_{j_m}$.
%A droite de l'égalité, $f_1$ apparait partout alors qu'à droite, il n'intervient pas. Ce qui est absurde car $\lambda \neq 0$.
\end{proof}

\section{Plongement de Plücker}
\begin{Def}[Plongement de Plücker]
On appelle plongement de Plücker l'application $\psi_{r,n} : G(r,n) \to P(\bigwedge^r k^n)$ qui envoie un sous-espace vectoriel $W$ de dimension $r$ de $k^n$ de base $e_1,\ldots,e_r$ sur $[e_1 \wedge \ldots \wedge e_r]$
\end{Def}
\begin{Cor}
$\psi_{r,n}$ ne dépend pas du choix de la base %et est injectif.
\end{Cor}

On va maintenant montrer l'injectivité de $\psi_{r,n}$ afin de justifier l'appellation "plongement"

\begin{Prop}
\label{InjPlucker}
Soit $W \in G(r,n)$ et $\{e_1,\ldots,e_r\}$ une base de $W$. Alors $W=\{x \in k^n |x \wedge (e_1 \wedge \ldots \wedge e_r)=0\}$ 
\end{Prop}
\begin{proof}
Posons $E_W=\{x \in k^n |x \wedge (e_1 \wedge \ldots \wedge e_r)=0\}$.
On remarque aisément que $W$ est un espace vectoriel et que $W \subset E_W$. \\
Soit $x \in E_W$ ie $x \wedge (e_1 \wedge \ldots \wedge e_r)=0$ et donc $\{x,e_1,\ldots,e_r\}$ est une famille liée. Comme $\{e_1,\ldots,e_r\}$ est une base alors $x \in \Vect(e_1,\ldots,e_r)=W$
\end{proof}
L'injectivité de $\psi_{r,n}$ en découle immédiatement. \\

 La réciproque de la proposition \ref{InjPlucker} est vraie :

\begin{Prop}
Soit $\Lambda \in \bigwedge^r k^n$ tel que $W=\{ x \in k^n | \Lambda \wedge x=0\}$ soit de dimension $r$. Alors $\psi_{r,n}(W)=[\Lambda]$
\end{Prop}
\begin{proof}
%Pour montrer ce lemme, on peut tout d'abord remarquer que l'action de $\GL_n(k)$ commute avec $\psi_{r,n}$. $\GL_n(k)$ agit sur $P(\bigwedge^r k^n$ par $P \cdot [e_1 \wedge \ldots \wedge e_r]=[Pe_1 \wedge \ldots \wedge Pe_r]$. De plus, comme $\{Pe_1,\ldots, Pe_r\}$ est une base de $W$ alors $\psi_{r,n}(PW)=P\cdot \psi_{r,n}(W)$. \\
Soit $\{e_1,\ldots,e_r\}$ une base de $W$. On peut compléter cette base en un base $\{e_1,\ldots,e_n\}$ de $k^n$. \\
On peut décomposer $\Lambda$ sur la base associée de $\bigwedge^r k^n$ : \\
$\Lambda=\sum_p \lambda_p e_{i_{1p}} \wedge \ldots \wedge e_{i_{rp}}$. \\
Comme $\Lambda \wedge e_i=0$ alors $\Lambda=\sum_{p,i_{jp} \neq 0} \lambda_p e_1 \wedge e_{i_{1p}} \wedge \ldots \wedge e_{i_{rp}}$. \\
Comme les $e_1 \wedge e_{i_{1p}} \wedge \ldots \wedge e_{i_{rp}}$ forment une famille libre alors les $\lambda_p$ correspondant sont nulles.
En itérant ce processus, on trouve que les scalaires correspondant aux multi-vecteurs contenant $e_j$ pour $j>r$ sont nuls et donc $\Lambda=\lambda e_1 \wedge \ldots \wedge e_r$, ce qu'il fallait démontrer.
\end{proof}

\subsection{L'image de $\psi_{r,n}$ comme variété algébrique projective}
Nous allons maintenant montrer que  $\psi_{r,n}(G(r,n))$ est donnée par des équations polynomiales . Pour cela, nous allons introduire quelques notations : 

\begin{Not}
On note $I(r,n)$ l'ensemble des sous-ensembles de $[\![1,n]\!]$ à $r$ éléments que l'on aura ordonné. \\
Pour tout $I=\{i_1 < \ldots <i_r\} \in I(r,n)$ et $P \in \GL_n(k)$,  $P \cdot [e_{i_1} \wedge \ldots \wedge e_{i_r}]=[Pe_{i_1} \wedge \ldots \wedge Pe_{i_r}]$. On étend l'action de $\GL_n(k)$ en tout point de $\bigwedge^r k^n$, linéairement. \\
On note, de la même façon, l'action de $\GL_n(k)$ sur la Grassmanienne $G(r,n)$ données par : $P \cdot W=P(W)$.\\
Pour tout $I,J \subset [\![1,n]\!]$, on note le nombre d'inversion $\iota(I,J)=\{(i,j) \in I \times J | i>j\}$. \\
Enfin, les polynômes de Plücker $\Pscr_{I,J}$, avec $I \in I(r-1,n)$ et $J=\{j_1 < \ldots < j_r\} \in I(r+1,n)$, sont données par : \\
$\Pscr_{I,J}(X)=\sum_{i,j_i \notin I} (-1)^{i+\iota(I,j_i)} X_{I \cup \{j_i\}}X_{J \setminus \{j_i\}}$ où $X=(X_K | K \in I(r,n))$
\end{Not}

\begin{Lemme}
Soient $W \in G(r,n)$, $\{v_1,\ldots,v_r\}$ une base de $W$ et $P=(v_1,\ldots,v_r) \in \Mcal_{n,r}(k)$. 
Alors $\psi_{r,n}(W)=\sum_{K=\{k_1<\ldots < k_r\}} \Delta_{K,[\![1,r]\!]}(P) e_{k_1} \wedge \ldots \wedge e_{k_r}$, \\
où pour $P=(p_{i,j})$, $\Delta_{I,J}(P)$ est le mineur $\det((p_{i,j})_{i \in I,j \in J})$
\end{Lemme}
\begin{proof}
Posons $v_i=(v_i^{(1)},\ldots,v_i^{(n)})$ pour tout $1 \leq i \leq n$. \\
$v_1 \wedge \ldots \wedge v_r=\sum_{k_1=1}^n \ldots \sum_{k_r=1}^n v_1^{(k_1)}\ldots v_r^{(k_r)} e_{k_1} \wedge \ldots \wedge e_{k_r}$\\
Par antilinéarité du produit extérieur, \\
$v_1 \wedge \ldots \wedge v_r=\sum_{K=\{k_1 < \ldots < k_r\}} \left(\sum_{\sigma \in \Sfrak_r} \varepsilon(\sigma) v_{\sigma(1)}^{(k_1)} \ldots v_{\sigma(r)}^{(k_r)} \right) e_{k_1} \wedge \ldots \wedge e_{k_r}$\\
\end{proof}
\begin{Lemme}
L'action de $\GL_n(k)$ commute avec $\psi_{r,n}$
\end{Lemme}
\begin{proof}
Si $V \in G(r,n)$, $\{v_1,\ldots,v_n\}$ est une base de $V$ et $P \in \GL_n(k)$ alors $\{Pv_1,\ldots,Pv_n\}$ est une base de $P\cdot V$
\end{proof}
\begin{Thm}
$\psi_{r,n}(G(r,n))=\{ v=(v_K)_{K \in I(r,n)} | \forall I \in I(r-1,n),\forall J \in I(r+1,n),\Pscr_{I,J}(v)=0\}$
\end{Thm}   

\begin{proof}
$\subset$ : Soient $w=w_1 \wedge \ldots \wedge w_r \in \psi_{r,n}(G(r,n))$, $A=(a_{ij})$ la matrice $(w_1,\ldots w_r)$, $L_i$ les lignes de $A$, $(\Delta_{K})_{K \in I(r,n)}$ les coordonnées de $W$ dans la base canonique de $\bigwedge^rW$. \\
Soit $I=\{i_1<\ldots<i_{r-1}$ et $j_i \in [\![1,n]\!]$. On considère $i_1<\ldots<j_i<\ldots <i_r$ où $I \cup j_r$ est écrit dans l'ordre croissant. \\
$(-1)^{\iota(I,j_i)} \Delta_{I \cup \{v_j\}}=(-1)^{\iota(I,j_i)}\det\begin{pmatrix} L_{i_1} \\ \vdots \\ L_{j_i} \\ \vdots \\ L_{i_{r-1}} \end{pmatrix}=\det\begin{pmatrix} L_{i_1}  \\ \vdots \\ L_{i_{r-1}}\\  L_{j_i} \end{pmatrix}$. \\
En développant sur la dernière ligne, \\
$(-1)^{\iota(I,j_i)} \Delta_{I \cup \{v_j\}}=\sum_{r=1}^s (-1)^{r+s}a_{j_i,s} \underbrace{\det((a_{i_l,m})_{1 \leq l \leq r-1,m \neq s})}_{=\Delta_{I,[\![1,r]\!] \setminus \{s\}}(A)=:\delta_s}$. \\
$\Pscr_{I,J}(w)=\sum_{i,j_i \notin I} (-1)^{i} \Delta_{J \setminus \{j_i\}} \sum_{s=1}^r a_{j_i,s}\delta_s=\sum_{s=1}^r(-1)^r\delta_s\sum_{i=1}^{r+1} (-1)^{i+s} \Delta_{J \setminus \{j_i\}}  a_{j_i,s}$. (on a pu étendre pour tous les éléments de $J$ car les valeurs supplémentaires sont nulles)\\
De plus, \\
$0=\det\begin{pmatrix}a_{j_1,1}& \ldots& a_{j_1,s} &a_{j_1,s} & \ldots a_{j_1,r} \\ \vdots&&&& \\ a_{j_{r+1},1}& \ldots& a_{j_{r+1},s} &a_{j_{r+1},s} & \ldots a_{j_{r+1},r} \end{pmatrix}=\sum_{i=1}^{r+1} (-1)^{k+s} \Delta_{J \setminus \{j_i\}}  a_{j_i,s}$ \\
D'où $\Pscr_{I,J}(v)=0$. \\
$\supset$ : Soit $[v]=[(v_K)_{K \in I(r,n)}] \in P(\bigwedge^r k^n)$ tel que : $\forall I \in I(r-1,n),\forall J \in I(r+1,n), \Pscr_{I,J}(v)=0$. \\
Soit $L=\{l_1<\ldots <l_r\}$ tel que $v_L \neq 0$. Soit $\sigma \in \Sfrak_n$ tel que $\sigma(l_i)=i$ croissante sur le complémentaire de $L$ et $P_\sigma$ la matrice de permutation associée. 
En développant les calculs sur le nombre d'inversions, on en déduit que : $\forall I \in I(r-1,n),\forall J \in I(r+1,n), \Pscr_{I,J}(P_\sigma(v))=\Pscr_{\sigma^{-1}(I),\sigma^{-1}(J)}(v)=0$. \\
Comme $\psi_{r,n}$ commute avec l'action de $\GL_n(k)$ alors on peut supposer $L=\{1,\ldots,r\}$. \\
Par homogénéité, on peut supposer $v_L=1$. \\
Soit $A=\begin{pmatrix} &&\\&I_r&\\&& \\ a_{r+1,1} & \ldots & a_{r+1,r} \\ \vdots & \vdots & \vdots \\ a_{n,1} & \ldots & a_{n,r}\end{pmatrix}$ où $\forall i \in [\![r+1,n]\!],\forall j \in [\![1,r]\!],a_{i,j}=(-1)^{r-j}v_{L \setminus \{j\} \cup \{i\}}$. \\
On va montrer que $\Delta_K(A)=v_K$ par récurrence sur le nombre d'éléments de $L \cap K^c$.
\begin{itemize}
	\item Si $Card(L \cap K^c)=0$ alors $K=L$ et $\Delta_K(A)=1=v_L$
	\item Si $Card( L \cap K^c)=1$ alors $K$ est de la forme $\{1<\ldots<i-1<i+1<\ldots<r<l\}$ et donc : \\
	$\Delta_K(A)=\det\begin{pmatrix} I_{i-1}& &0_{r-i+1}\\0_r &&I_{n-r} \\ a_{k,1} & \ldots & a_{k,r} \end{pmatrix}=(-1)^{r+1} a_{li}=v_{L \setminus \{l\} \cup \{i\}}=v_K$
	\item Soit $s\geq 2$ et supposons la propriété vraie au rang $s-1$. Soit $K=\{k_1<\ldots <k_r\}$ tel que $Card(L \cap K^c)=s$. \\
	Alors,en développant le déterminant sur la $k_r$ ème ligne,\\ $\Delta_K(A)=\sum_{i=1}^{r} (-1)^{r+i} a_{k_r,i} \underbrace{\Delta_{K \setminus \{k_r\},[\![1,r]\!] \setminus \{i\}}(A)}_{=: m_{k_r,i}}$. \\
	De plus, si $1 \leq i \leq r$, on obtient, en développant par rapport à la $i$ème ligne (qui est de la forme $( 0 \ldots 1,0\ldots 0)$ où le 1 est à la $i$ème ligne),   \\
	$\Delta_{I \cup \{i\}}=(-1)^{\iota(I,i)}(-1)^{r+i}m_{k_r,i}$\; ( le $(-1)^{\iota(I,i)}$ permet de réordonner la matrice) \\ 
		où $I=K \setminus \{k_r\}$  \\
	Comme $Card(L \cap (I\cup \{i\})^c)=s-1$ alors, par hypothèse de récurrence, $v_{I \cup \{i\}}=\Delta_{I \cup \{i\}}$. On a donc: \\
	$\Delta_K(A)=\sum_{i=1}^{r} (-1)^{r+i} a_{k_r,i}m_{k_r,i}=\sum_{i=1}^{r} (-1)^{r+i}(-1)^{\iota(I,i)}(-1)^{r+i} a_{k_r,i}\Delta_{I \cup \{i\}}$. \\
	$\Delta_K(A)=\sum_{i=1}^{r} (-1)^{r+i}(-1)^{\iota(I,i)} \Delta_{J \setminus \{i\}}\Delta_{I \cup \{i\}}$ où $J=\{1,\ldots,r,k_r\}$ \\
	$\Delta_K(A)=(-1)^r\Pscr_{I,J}(v)+ v_Kv_L=v_K$ car $v_L=1$\\
	Si on note $v_1,v_r$ les colonnes de $A$ alors $v=\psi_{r,n}(\Vect(v_1,\ldots,v_r))$.
\end{itemize}
\end{proof}                                                                   























\ifwhole
 \end{document}
\fi